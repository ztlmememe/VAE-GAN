{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "# import inception_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(img_syn, 'img_syn.pt')\n",
    "# torch.save(label_syn, 'label_syn.pt')\n",
    "# # 读取tensor\n",
    "img_syn = torch.load('img_syn.pt')\n",
    "label_syn = torch.load('label_syn.pt')\n",
    "\n",
    "device  = \"cuda:0\"\n",
    "img_syn =img_syn.to(device)\n",
    "label_syn =label_syn.to(device)\n",
    "device = img_syn.device\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='Parameter Processing')\n",
    "parser.add_argument('--dataset', type=str, default='CIFAR10', help='dataset')\n",
    "parser.add_argument('--model', type=str, default='ConvNet', help='model')\n",
    "parser.add_argument('--ipc', type=int, default=50, help='image(s) per class')\n",
    "parser.add_argument('--eval_mode', type=str, default='SS', help='eval_mode') # S: the same to training model, M: multi architectures,  W: net width, D: net depth, A: activation function, P: pooling layer, N: normalization layer,\n",
    "parser.add_argument('--num_exp', type=int, default=1, help='the number of experiments')\n",
    "parser.add_argument('--num_eval', type=int, default=1, help='the number of evaluating randomly initialized models')\n",
    "parser.add_argument('--epoch_eval_train', type=int, default=1000, help='epochs to train a model with synthetic data') # it can be small for speeding up with little performance drop\n",
    "parser.add_argument('--Iteration', type=int, default=2000, help='training iterations')\n",
    "parser.add_argument('--lr_img', type=float, default=1.0, help='learning rate for updating synthetic images')\n",
    "parser.add_argument('--lr_net', type=float, default=0.01, help='learning rate for updating network parameters')\n",
    "parser.add_argument('--batch_real', type=int, default=256, help='batch size for real data')\n",
    "parser.add_argument('--batch_train', type=int, default=256, help='batch size for training networks')\n",
    "parser.add_argument('--init', type=str, default='real', help='noise/real: initialize synthetic images from random noise or randomly sampled real images.')\n",
    "parser.add_argument('--dsa_strategy', type=str, default='color_crop_cutout_flip_scale_rotate', help='differentiable Siamese augmentation strategy')\n",
    "parser.add_argument('--data_path', type=str, default='/home/ssd7T/ZTL_gcond/data_cv', help='dataset path')\n",
    "parser.add_argument('--save_path', type=str, default='result/gen', help='path to save results')\n",
    "parser.add_argument('--dis_metric', type=str, default='ours', help='distance metric')\n",
    "from utils import get_loops, get_dataset, get_network, get_eval_pool, evaluate_synset, get_daparam, match_loss, get_time, TensorDataset, epoch, DiffAugment, ParamDiffAug\n",
    "import warnings\n",
    "args = parser.parse_args([])\n",
    "channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = get_dataset(args.dataset, args.data_path)\n",
    "\n",
    "images_all = [torch.unsqueeze(dst_train[i][0], dim=0) for i in range(len(dst_train))]\n",
    "labels_all = [dst_train[i][1] for i in range(len(dst_train))]\n",
    "indices_class = [[] for c in range(num_classes)]\n",
    "for i, lab in enumerate(labels_all):\n",
    "    indices_class[lab].append(i)\n",
    "images_all = torch.cat(images_all, dim=0).to(device)\n",
    "labels_all = torch.tensor(labels_all, dtype=torch.long, device=device)\n",
    "\n",
    "accs = []\n",
    "model_eval_pool = get_eval_pool(args.eval_mode, args.model, args.model)\n",
    "args.device = \"cuda:0\"\n",
    "import copy\n",
    "accs_all_exps = dict() # record performances of all experiments\n",
    "for key in model_eval_pool:\n",
    "    accs_all_exps[key] = []\n",
    "args.dsa_param = ParamDiffAug()\n",
    "args.dsa = False if args.dsa_strategy in ['none', 'None'] else True\n",
    "model_eval= model_eval_pool[0]\n",
    "data_save = []\n",
    "\n",
    "# img_real = []\n",
    "# label_real = []\n",
    "# for c in range(num_classes):\n",
    "#     idx_shuffle = np.random.permutation(indices_class[c])\n",
    "#     img_real.append(images_all[idx_shuffle].to(\"cpu\") )\n",
    "#     label_real.append(labels_all[idx_shuffle].to(\"cpu\"))\n",
    "# img_real = torch.from_numpy(np.concatenate(img_real, axis=0))\n",
    "# label_real = torch.from_numpy(np.concatenate(label_real, axis=0))\n",
    "\n",
    "\n",
    "SEED = 114514\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "img_real_train = []\n",
    "label_real_train = []\n",
    "for c in range(num_classes):\n",
    "    idx_shuffle = np.random.permutation(indices_class[c])[:50]\n",
    "    img_real_train.append(images_all[idx_shuffle].to(\"cpu\") )\n",
    "    label_real_train.append(labels_all[idx_shuffle].to(\"cpu\"))\n",
    "img_real_train = torch.from_numpy(np.concatenate(img_real_train, axis=0))\n",
    "label_real_train = torch.from_numpy(np.concatenate(label_real_train, axis=0))\n",
    "\n",
    "SEED = 87\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "img_real_test = []\n",
    "label_real_test = []\n",
    "for c in range(num_classes):\n",
    "    idx_shuffle = np.random.permutation(indices_class[c])[:50]\n",
    "    img_real_test.append(images_all[idx_shuffle].to(\"cpu\") )\n",
    "    label_real_test.append(labels_all[idx_shuffle].to(\"cpu\"))\n",
    "img_real_test = torch.from_numpy(np.concatenate(img_real_test, axis=0))\n",
    "label_real_test = torch.from_numpy(np.concatenate(label_real_test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "batch = 50\n",
    "num_feat = 3072\n",
    "batch_size = 64\n",
    "z_dim = 100\n",
    "num_epochs = 100\n",
    "\n",
    "epochs = 100\n",
    "# Hyper-parameters \n",
    "latent_size = 32\n",
    "batch_size = 50\n",
    "lr_D = 0.0002\n",
    "lr_G = 0.001\n",
    "lr_Q = 0.0001\n",
    "beta1 = 0.5\n",
    "\n",
    "# Create target dataloader\n",
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "           ])\n",
    "\n",
    "# Discriminator\n",
    "D = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1), \n",
    "    nn.LeakyReLU(0.2),\n",
    "    \n",
    "    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    \n",
    "    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.LeakyReLU(0.2),\n",
    "\n",
    "    nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=0),\n",
    "    nn.BatchNorm2d(512),\n",
    "    nn.LeakyReLU(0.2),\n",
    "\n",
    "    nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "D = D.to(device) \n",
    "\n",
    "# Generator \n",
    "G = nn.Sequential(\n",
    "    nn.Linear(3*32*32, 256*4*4), \n",
    "    nn.BatchNorm1d(256*4*4),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "    nn.BatchNorm2d(64),  \n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
    "    nn.Tanh()\n",
    ")\n",
    "G = G.to(device)\n",
    "\n",
    "# Losses\n",
    "criterion = nn.BCELoss()\n",
    "d_optimizer = optim.Adam(D.parameters(), lr=lr_D, betas=(beta1, 0.999))\n",
    "g_optimizer = optim.Adam(G.parameters(), lr=lr_G, betas=(beta1, 0.999))\n",
    "\n",
    "def gradient_penalty(real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = torch.randn((batch_size, 1, 1, 1)).to(device)\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = torch.ones(d_interpolates.size()).requires_grad_(False).to(device)\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4800x32 and 3072x4096)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_dgan.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_dgan.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m batch_img_syn_label \u001b[39m=\u001b[39m label_syn[c\u001b[39m*\u001b[39mbatch_size:(c\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mbatch_size]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_dgan.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Sample noise as generator input\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_dgan.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_size)))).to(device)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_dgan.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# z = batch_img\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_dgan.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_dgan.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Generate a batch of images .view(batch_img.size(0), -1)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_dgan.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m gen_imgs \u001b[39m=\u001b[39m G(batch_img)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_dgan.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Loss measures generator's ability to fool the discriminator\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_dgan.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m g_loss \u001b[39m=\u001b[39m criterion(D(gen_imgs), batch_img_syn_label)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4800x32 and 3072x4096)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "IS = []\n",
    "for epoch in range(epochs):    \n",
    "    for c in range(num_classes):\n",
    "    \n",
    "        batch_img = img_real_train[c*batch_size:(c+1)*batch_size].reshape((batch, 3, 32, 32)).to(device).to(device)\n",
    "        batch_img_label = label_real_train[c*batch_size:(c+1)*batch_size].to(device)\n",
    "        batch_img_syn = img_syn[c*batch_size:(c+1)*batch_size].reshape((batch, 3, 32, 32)).to(device).to(device)\n",
    "        batch_img_syn_label = label_syn[c*batch_size:(c+1)*batch_size].to(device)\n",
    "\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        # z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_size)))).to(device)\n",
    "        # z = batch_img\n",
    "\n",
    "        # Generate a batch of images .view(batch_img.size(0), -1)\n",
    "        gen_imgs = G(batch_img)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = criterion(D(gen_imgs), batch_img_syn_label)\n",
    "        \n",
    "        # Real images \n",
    "        real_out = D(batch_img_syn)\n",
    "        d_real_loss = criterion(real_out, torch.ones_like(real_out)) \n",
    "        # Fake images\n",
    "        fake_out = D(gen_imgs.detach())\n",
    "        d_fake_loss = criterion(fake_out, torch.zeros_like(fake_out))\n",
    "        # Gradient penalty\n",
    "        gradient_penalty_val = gradient_penalty(batch_img_syn , gen_imgs.data)\n",
    "        # Total discriminator loss\n",
    "        d_loss = d_real_loss + d_fake_loss + gradient_penalty_val\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator \n",
    "        # ---------------------\n",
    "\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # --------------\n",
    "        # Log Progress\n",
    "        # --------------\n",
    "\n",
    "        print (\"[Epoch %d/%d][D loss: %f] [G loss: %f]\" % (epoch, epochs, \n",
    "                                                            d_loss.item(), g_loss.item())) \n",
    "                                                            \n",
    "        \n",
    "        # batches_done = epoch * len(data_loader) + i\n",
    "        # if batches_done % 100 == 0:\n",
    "        #     # Calculate Inception Score\n",
    "        #     z = Variable(Tensor(np.random.normal(0, 1, (5000, latent_size)))).to(device) \n",
    "        #     gen_imgs = G(z)\n",
    "        #     IS.append(inception_score(gen_imgs.cpu().data, cuda=False, batch_size=32, \n",
    "        #                             resize=True, splits=10)[0])\n",
    "\n",
    "        #     # Save generated images \n",
    "        #     save_image(gen_imgs.data[:25], \"%d.png\" % batches_done, nrow=5, normalize=True)\n",
    "            \n",
    "# Plot IS \n",
    "# plot(IS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ztl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
