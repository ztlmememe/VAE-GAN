{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_it_pool:  [0, 2000]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "================== Exp 0 ==================\n",
      " \n",
      "Hyper-parameters: \n",
      " {'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 50, 'eval_mode': 'SS', 'num_exp': 1, 'num_eval': 1, 'epoch_eval_train': 1000, 'Iteration': 2000, 'lr_img': 1.0, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'real', 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'data_path': '/home/ssd7T/ZTL_gcond/data_cv', 'save_path': 'result/gen', 'dis_metric': 'ours', 'method': 'DM', 'outer_loop': 50, 'inner_loop': 10, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7f143069d340>, 'dsa': True}\n",
      "Evaluation model pool:  ['ConvNet']\n",
      "class c = 0: 5000 real images\n",
      "class c = 1: 5000 real images\n",
      "class c = 2: 5000 real images\n",
      "class c = 3: 5000 real images\n",
      "class c = 4: 5000 real images\n",
      "class c = 5: 5000 real images\n",
      "class c = 6: 5000 real images\n",
      "class c = 7: 5000 real images\n",
      "class c = 8: 5000 real images\n",
      "class c = 9: 5000 real images\n",
      "real images channel 0, mean = -0.0000, std = 1.2211\n",
      "real images channel 1, mean = -0.0002, std = 1.2211\n",
      "real images channel 2, mean = 0.0002, std = 1.3014\n",
      "initialize synthetic data from random real images\n",
      "[2023-10-09 00:12:10] training begins\n",
      "-------------------------\n",
      "Evaluation\n",
      "model_train = ConvNet, model_eval = ConvNet, iteration = 0\n",
      "DSA augmentation strategy: \n",
      " color_crop_cutout_flip_scale_rotate\n",
      "DSA augmentation parameters: \n",
      " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
      "[2023-10-09 00:15:45] Evaluate_00: epoch = 1000 train time = 208 s train loss = 0.004877 train acc = 1.0000, test acc = 0.5057\n",
      "Evaluate 1 random ConvNet, mean = 0.5057 std = 0.0000\n",
      "-------------------------\n",
      "[2023-10-09 00:15:46] iter = 00000, loss = 5.5401\n",
      "[2023-10-09 00:15:53] iter = 00010, loss = 5.4554\n",
      "[2023-10-09 00:15:59] iter = 00020, loss = 4.7342\n",
      "[2023-10-09 00:16:06] iter = 00030, loss = 4.3806\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=168'>169</a>\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0.0\u001b[39m)\u001b[39m.\u001b[39mto(args\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=169'>170</a>\u001b[0m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_classes):\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=170'>171</a>\u001b[0m     img_real \u001b[39m=\u001b[39m get_images(c, args\u001b[39m.\u001b[39;49mbatch_real)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=171'>172</a>\u001b[0m     img_syn \u001b[39m=\u001b[39m image_syn[c\u001b[39m*\u001b[39margs\u001b[39m.\u001b[39mipc:(c\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39margs\u001b[39m.\u001b[39mipc]\u001b[39m.\u001b[39mreshape((args\u001b[39m.\u001b[39mipc, channel, im_size[\u001b[39m0\u001b[39m], im_size[\u001b[39m1\u001b[39m]))\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=173'>174</a>\u001b[0m     \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mdsa:\n",
      "\u001b[1;32m/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen.ipynb Cell 1\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_images\u001b[39m(c, n): \u001b[39m# get random n images from class c\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m     idx_shuffle \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mpermutation(indices_class[c])[:n]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m images_all[idx_shuffle]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import save_image\n",
    "from utils import get_loops, get_dataset, get_network, get_eval_pool, evaluate_synset, get_daparam, match_loss, get_time, TensorDataset, epoch, DiffAugment, ParamDiffAug\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Parameter Processing')\n",
    "parser.add_argument('--dataset', type=str, default='CIFAR10', help='dataset')\n",
    "parser.add_argument('--model', type=str, default='ConvNet', help='model')\n",
    "parser.add_argument('--ipc', type=int, default=50, help='image(s) per class')\n",
    "parser.add_argument('--eval_mode', type=str, default='SS', help='eval_mode') # S: the same to training model, M: multi architectures,  W: net width, D: net depth, A: activation function, P: pooling layer, N: normalization layer,\n",
    "parser.add_argument('--num_exp', type=int, default=1, help='the number of experiments')\n",
    "parser.add_argument('--num_eval', type=int, default=1, help='the number of evaluating randomly initialized models')\n",
    "parser.add_argument('--epoch_eval_train', type=int, default=1000, help='epochs to train a model with synthetic data') # it can be small for speeding up with little performance drop\n",
    "parser.add_argument('--Iteration', type=int, default=2000, help='training iterations')\n",
    "parser.add_argument('--lr_img', type=float, default=1.0, help='learning rate for updating synthetic images')\n",
    "parser.add_argument('--lr_net', type=float, default=0.01, help='learning rate for updating network parameters')\n",
    "parser.add_argument('--batch_real', type=int, default=256, help='batch size for real data')\n",
    "parser.add_argument('--batch_train', type=int, default=256, help='batch size for training networks')\n",
    "parser.add_argument('--init', type=str, default='real', help='noise/real: initialize synthetic images from random noise or randomly sampled real images.')\n",
    "parser.add_argument('--dsa_strategy', type=str, default='color_crop_cutout_flip_scale_rotate', help='differentiable Siamese augmentation strategy')\n",
    "parser.add_argument('--data_path', type=str, default='/home/ssd7T/ZTL_gcond/data_cv', help='dataset path')\n",
    "parser.add_argument('--save_path', type=str, default='result/gen', help='path to save results')\n",
    "parser.add_argument('--dis_metric', type=str, default='ours', help='distance metric')\n",
    "\n",
    "args = parser.parse_args([])\n",
    "args.method = 'DM'\n",
    "args.outer_loop, args.inner_loop = get_loops(args.ipc)\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args.dsa_param = ParamDiffAug()\n",
    "args.dsa = False if args.dsa_strategy in ['none', 'None'] else True\n",
    "\n",
    "if not os.path.exists(args.data_path):\n",
    "    os.mkdir(args.data_path)\n",
    "\n",
    "if not os.path.exists(args.save_path):\n",
    "    os.mkdir(args.save_path)\n",
    "\n",
    "eval_it_pool = np.arange(0, args.Iteration+1, 2000).tolist() if args.eval_mode == 'S' or args.eval_mode == 'SS' else [args.Iteration] # The list of iterations when we evaluate models and record results.\n",
    "print('eval_it_pool: ', eval_it_pool)\n",
    "channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = get_dataset(args.dataset, args.data_path)\n",
    "model_eval_pool = get_eval_pool(args.eval_mode, args.model, args.model)\n",
    "\n",
    "\n",
    "accs_all_exps = dict() # record performances of all experiments\n",
    "for key in model_eval_pool:\n",
    "    accs_all_exps[key] = []\n",
    "\n",
    "data_save = []\n",
    "\n",
    "pairs_real = []\n",
    "indexs_real = []\n",
    "for exp in range(args.num_exp):\n",
    "    print('\\n================== Exp %d ==================\\n '%exp)\n",
    "    print('Hyper-parameters: \\n', args.__dict__)\n",
    "    print('Evaluation model pool: ', model_eval_pool)\n",
    "\n",
    "    ''' organize the real dataset '''\n",
    "    images_all = []\n",
    "    labels_all = []\n",
    "    indices_class = [[] for c in range(num_classes)]\n",
    "\n",
    "    images_all = [torch.unsqueeze(dst_train[i][0], dim=0) for i in range(len(dst_train))]\n",
    "    labels_all = [dst_train[i][1] for i in range(len(dst_train))]\n",
    "    for i, lab in enumerate(labels_all):\n",
    "        indices_class[lab].append(i)\n",
    "    images_all = torch.cat(images_all, dim=0).to(args.device)\n",
    "    labels_all = torch.tensor(labels_all, dtype=torch.long, device=args.device)\n",
    "\n",
    "\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        print('class c = %d: %d real images'%(c, len(indices_class[c])))\n",
    "\n",
    "    def get_images(c, n): # get random n images from class c\n",
    "        idx_shuffle = np.random.permutation(indices_class[c])[:n]\n",
    "        return images_all[idx_shuffle]\n",
    "    \n",
    "    def get_images_init(c, n,exp): # get random n images from class c\n",
    "        # start_idx = i  # 指定起始索引 i\n",
    "        # end_idx = i + n  # 计算结束索引（不包括结束索引）\n",
    "\n",
    "        # 从指定的起始索引到结束索引获取元素\n",
    "        idx_shuffle  = indices_class[c][exp:exp + n]\n",
    "\n",
    "        # idx_shuffle = np.random.permutation(indices_class[c])[:n]\n",
    "        return images_all[idx_shuffle],idx_shuffle\n",
    "\n",
    "    for ch in range(channel):\n",
    "        print('real images channel %d, mean = %.4f, std = %.4f'%(ch, torch.mean(images_all[:, ch]), torch.std(images_all[:, ch])))\n",
    "\n",
    "\n",
    "    ''' initialize the synthetic data '''\n",
    "    image_syn = torch.randn(size=(num_classes*args.ipc, channel, im_size[0], im_size[1]), dtype=torch.float, requires_grad=True, device=args.device)\n",
    "    label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]\n",
    "\n",
    "    if args.init == 'real':\n",
    "        print('initialize synthetic data from random real images')\n",
    "        for c in range(num_classes):\n",
    "            reals,index = get_images_init(c, args.ipc,exp)\n",
    "            reals = reals.detach().data\n",
    "            pairs_real.append(reals)\n",
    "            indexs_real.append(index)\n",
    "            image_syn.data[c*args.ipc:(c+1)*args.ipc] = reals\n",
    "            # pairs_real\n",
    "    else:\n",
    "        print('initialize synthetic data from random noise')\n",
    "    \n",
    "    # init_syn = image_syn.detach()\n",
    "    torch.save(pairs_real, 'pairs_real_{exp}.pt')\n",
    "    torch.save(indexs_real, 'indexs_real_{exp}.pt')\n",
    "\n",
    "\n",
    "    ''' training '''\n",
    "    optimizer_img = torch.optim.SGD([image_syn, ], lr=args.lr_img, momentum=0.5) # optimizer_img for synthetic data\n",
    "    optimizer_img.zero_grad()\n",
    "    print('%s training begins'%get_time())\n",
    "\n",
    "    for it in range(args.Iteration+1):\n",
    "\n",
    "        ''' Evaluate synthetic data '''\n",
    "        if it in eval_it_pool:\n",
    "            for model_eval in model_eval_pool:\n",
    "                print('-------------------------\\nEvaluation\\nmodel_train = %s, model_eval = %s, iteration = %d'%(args.model, model_eval, it))\n",
    "\n",
    "                print('DSA augmentation strategy: \\n', args.dsa_strategy)\n",
    "                print('DSA augmentation parameters: \\n', args.dsa_param.__dict__)\n",
    "\n",
    "                accs = []\n",
    "                for it_eval in range(args.num_eval):\n",
    "                    net_eval = get_network(model_eval, channel, num_classes, im_size).to(args.device) # get a random model\n",
    "                    image_syn_eval, label_syn_eval = copy.deepcopy(image_syn.detach()), copy.deepcopy(label_syn.detach()) # avoid any unaware modification\n",
    "                    _, acc_train, acc_test = evaluate_synset(it_eval, net_eval, image_syn_eval, label_syn_eval, testloader, args)\n",
    "                    accs.append(acc_test)\n",
    "                print('Evaluate %d random %s, mean = %.4f std = %.4f\\n-------------------------'%(len(accs), model_eval, np.mean(accs), np.std(accs)))\n",
    "\n",
    "                if it == args.Iteration: # record the final results\n",
    "                    accs_all_exps[model_eval] += accs\n",
    "\n",
    "            ''' visualize and save '''\n",
    "            save_name = os.path.join(args.save_path, 'vis_%s_%s_%s_%dipc_exp%d_iter%d.png'%(args.method, args.dataset, args.model, args.ipc, exp, it))\n",
    "            image_syn_vis = copy.deepcopy(image_syn.detach().cpu())\n",
    "            for ch in range(channel):\n",
    "                image_syn_vis[:, ch] = image_syn_vis[:, ch]  * std[ch] + mean[ch]\n",
    "            image_syn_vis[image_syn_vis<0] = 0.0\n",
    "            image_syn_vis[image_syn_vis>1] = 1.0\n",
    "            save_image(image_syn_vis, save_name, nrow=args.ipc) # Trying normalize = True/False may get better visual effects.\n",
    "\n",
    "\n",
    "\n",
    "        ''' Train synthetic data '''\n",
    "        net = get_network(args.model, channel, num_classes, im_size).to(args.device) # get a random model\n",
    "        net.train()\n",
    "        for param in list(net.parameters()):\n",
    "            param.requires_grad = False\n",
    "\n",
    "        embed = net.module.embed if torch.cuda.device_count() > 1 else net.embed # for GPU parallel\n",
    "\n",
    "        loss_avg = 0\n",
    "\n",
    "        ''' update synthetic data '''\n",
    "        if 'BN' not in args.model: # for ConvNet\n",
    "            loss = torch.tensor(0.0).to(args.device)\n",
    "            for c in range(num_classes):\n",
    "                img_real = get_images(c, args.batch_real)\n",
    "                img_syn = image_syn[c*args.ipc:(c+1)*args.ipc].reshape((args.ipc, channel, im_size[0], im_size[1]))\n",
    "\n",
    "                if args.dsa:\n",
    "                    seed = int(time.time() * 1000) % 100000\n",
    "                    img_real = DiffAugment(img_real, args.dsa_strategy, seed=seed, param=args.dsa_param)\n",
    "                    img_syn = DiffAugment(img_syn, args.dsa_strategy, seed=seed, param=args.dsa_param)\n",
    "\n",
    "                output_real = embed(img_real).detach()\n",
    "                output_syn = embed(img_syn)\n",
    "\n",
    "                loss += torch.sum((torch.mean(output_real, dim=0) - torch.mean(output_syn, dim=0))**2)\n",
    "\n",
    "        else: # for ConvNetBN\n",
    "            images_real_all = []\n",
    "            images_syn_all = []\n",
    "            loss = torch.tensor(0.0).to(args.device)\n",
    "            for c in range(num_classes):\n",
    "                img_real = get_images(c, args.batch_real)\n",
    "                img_syn = image_syn[c*args.ipc:(c+1)*args.ipc].reshape((args.ipc, channel, im_size[0], im_size[1]))\n",
    "\n",
    "                if args.dsa:\n",
    "                    seed = int(time.time() * 1000) % 100000\n",
    "                    img_real = DiffAugment(img_real, args.dsa_strategy, seed=seed, param=args.dsa_param)\n",
    "                    img_syn = DiffAugment(img_syn, args.dsa_strategy, seed=seed, param=args.dsa_param)\n",
    "\n",
    "                images_real_all.append(img_real)\n",
    "                images_syn_all.append(img_syn)\n",
    "\n",
    "            images_real_all = torch.cat(images_real_all, dim=0)\n",
    "            images_syn_all = torch.cat(images_syn_all, dim=0)\n",
    "\n",
    "            output_real = embed(images_real_all).detach()\n",
    "            output_syn = embed(images_syn_all)\n",
    "\n",
    "            loss += torch.sum((torch.mean(output_real.reshape(num_classes, args.batch_real, -1), dim=1) - torch.mean(output_syn.reshape(num_classes, args.ipc, -1), dim=1))**2)\n",
    "\n",
    "\n",
    "\n",
    "        optimizer_img.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_img.step()\n",
    "        loss_avg += loss.item()\n",
    "\n",
    "\n",
    "        loss_avg /= (num_classes)\n",
    "\n",
    "        if it%10 == 0:\n",
    "            print('%s iter = %05d, loss = %.4f' % (get_time(), it, loss_avg))\n",
    "\n",
    "        if it == args.Iteration: # only record the final results\n",
    "            # img_syn = image_syn.detach().to(device)\n",
    "            # label_syn = label_syn.to(device)\n",
    "            torch.save(image_syn.detach().cpu(), f'img_syn_{exp}.pt')\n",
    "            torch.save(label_syn.detach().cpu(), f'label_syn_{exp}.pt')\n",
    "\n",
    "            # data_save.append([copy.deepcopy(image_syn.detach().cpu()), copy.deepcopy(label_syn.detach().cpu())])\n",
    "            # torch.save({'data': data_save, 'accs_all_exps': accs_all_exps, }, os.path.join(args.save_path, 'res_%s_%s_%s_%dipc.pt'%(args.method, args.dataset, args.model, args.ipc)))\n",
    "\n",
    "\n",
    "print('\\n==================== Final Results ====================\\n')\n",
    "for key in model_eval_pool:\n",
    "    accs = accs_all_exps[key]\n",
    "    print('Run %d experiments, train on %s, evaluate %d random %s, mean  = %.2f%%  std = %.2f%%'%(args.num_exp, args.model, len(accs), key, np.mean(accs)*100, np.std(accs)*100))\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = img_syn.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_syn = image_syn.detach().to(device)\n",
    "label_syn = label_syn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 3, 32, 32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_syn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 3, 32, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_real[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_tensor = torch.cat(pairs_real, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 3, 32, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29,\n",
       " 30,\n",
       " 35,\n",
       " 49,\n",
       " 77,\n",
       " 93,\n",
       " 115,\n",
       " 116,\n",
       " 129,\n",
       " 165,\n",
       " 179,\n",
       " 185,\n",
       " 189,\n",
       " 199,\n",
       " 213,\n",
       " 220,\n",
       " 223,\n",
       " 233,\n",
       " 264,\n",
       " 276,\n",
       " 279,\n",
       " 284,\n",
       " 293,\n",
       " 308,\n",
       " 317,\n",
       " 332,\n",
       " 341,\n",
       " 344,\n",
       " 348,\n",
       " 349,\n",
       " 352,\n",
       " 371,\n",
       " 373,\n",
       " 376,\n",
       " 392,\n",
       " 401,\n",
       " 404,\n",
       " 405,\n",
       " 407,\n",
       " 415,\n",
       " 417,\n",
       " 436,\n",
       " 439,\n",
       " 448,\n",
       " 453,\n",
       " 455,\n",
       " 457,\n",
       " 467,\n",
       " 468,\n",
       " 481]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexs_real[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(img_syn, 'img_syn.pt')\n",
    "torch.save(label_syn, 'label_syn.pt')\n",
    "torch.save(pairs_real, 'pairs_real.pt')\n",
    "torch.save(indexs_real, 'indexs_real.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs_real = []\n",
    "# indexs_real = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_syn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.rand(3,3)\n",
    "# torch.save(x, 'tensor.pt')\n",
    "\n",
    "# # 读取tensor\n",
    "# x = torch.load('tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义自动编码器\n",
    "class Autoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,num_feat,num_classes = 10):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # 输入图像通道数修改为3\n",
    "        self.encoder = nn.Sequential(  \n",
    "            nn.Conv2d(3, 64, 3, stride=2, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 7)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        # self.aux_classifier = nn.Linear(32, 10) \n",
    "        self.classifier = nn.Linear(num_feat, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 50\n",
    "num_feat = 3072\n",
    "model = Autoencoder(num_feat).to(device)  \n",
    "criterion = nn.MSELoss().to(device)  \n",
    "# aux loss函数\n",
    "aux_loss_fn = nn.CrossEntropyLoss().to(args.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 训练\n",
    "for epoch in range(1000):\n",
    "  # for i in range(0, len(img_syn), batch):\n",
    "  #   batch_img = img_syn[i:i+batch] \n",
    "  #   batch_label = label_syn[i:i+batch] \n",
    "  #   batch_img = batch_img.to(device) \n",
    "  #   recon = model(batch_img)\n",
    "  #   loss = criterion(recon, batch_img)\n",
    "\n",
    "  #   # 计算辅助分类损失\n",
    "  #   classifiation = model.classifier(recon.view(recon.size(0), -1)) \n",
    "  #   # aux_loss = aux_loss_fn(aux_logits, batch_label)\n",
    "  #   cls_loss = aux_loss_fn(classifiation, batch_label)\n",
    "    \n",
    "  #   total_loss = 0.9 *loss + 0.1 * cls_loss\n",
    "  #   print(\"mse:\",loss.item())\n",
    "  #   print(\"cls_loss:\",cls_loss.item())\n",
    "  #   # if total_loss < 0.3:\n",
    "  #   #   break\n",
    "    \n",
    "  #   optimizer.zero_grad()\n",
    "  #   loss.backward()\n",
    "  #   optimizer.step() \n",
    "  # for i in range(0, len(img_syn), batch):\n",
    "  total_loss = 0\n",
    "  for c in range(num_classes):\n",
    "                # 获取类别c的合成图像和类别中心\n",
    "                # image_syn[c*args.ipc:(c+1)*args.ipc].reshape((args.ipc, channel, im_size[0], im_size[1]))\n",
    "    batch_img = image_syn[c*batch:(c+1)*batch].reshape((batch, 3, 32, 32)) \n",
    "    batch_label = label_syn[c*batch:(c+1)*batch] \n",
    "    batch_img = batch_img.to(device) \n",
    "    recon = model(batch_img)\n",
    "    loss = criterion(recon, batch_img)\n",
    "\n",
    "    # 计算辅助分类损失\n",
    "    classifiation = model.classifier(recon.view(recon.size(0), -1)) \n",
    "    # aux_loss = aux_loss_fn(aux_logits, batch_label)\n",
    "    cls_loss = aux_loss_fn(classifiation, batch_label)\n",
    "    \n",
    "    total_loss += 0.4 *loss + 0.6 * cls_loss\n",
    "    # print(total_loss.item())\n",
    "    # total_loss = 0.9 *loss + 0.1 * cls_loss\n",
    "    # print(\"mse:\",loss.item())\n",
    "    # print(\"cls_loss:\",cls_loss.item())\n",
    "  if total_loss.item()<18.87:\n",
    "\n",
    "      print(\"done\")\n",
    "      break\n",
    "  # print(total_loss.item())\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step() \n",
    "  \n",
    "    \n",
    "# 生成新图像\n",
    "noise = torch.randn(500, 3, 32, 32).to(device) \n",
    "with torch.no_grad():\n",
    "  output = model.decoder(image_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  output = model(img_real.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-06 23:52:01] Evaluate_00: epoch = 1000 train time = 67 s train loss = 0.027192 train acc = 0.9980, test acc = 0.2098\n",
      "\n",
      "Evaluate 1 random ConvNet, mean = 0.2098 std = 0.0000\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "\n",
    "net_eval = get_network(model_eval, channel, num_classes, im_size).to(args.device) # get a random model\n",
    "image_syn_eval, label_syn_eval = copy.deepcopy(output), copy.deepcopy(label_syn) # avoid any unaware modification\n",
    "_, acc_train, acc_test = evaluate_synset(it_eval, net_eval, image_syn_eval, label_syn_eval, testloader, args)\n",
    "accs.append(acc_test)\n",
    "print()\n",
    "print('Evaluate %d random %s, mean = %.4f std = %.4f\\n-------------------------'%(len(accs), model_eval, np.mean(accs), np.std(accs)))\n",
    "# [2023-10-06 20:45:24] Evaluate_00: epoch = 1000 train time = 61 s train loss = 0.000965 train acc = 1.0000, test acc = 0.5001\n",
    "# Evaluate 1 random ConvNet, mean = 0.5001 std = 0.0000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_syn_vis = copy.deepcopy(output.detach().cpu())\n",
    "from torchvision.utils import save_image\n",
    "image_syn_vis[image_syn_vis<0] = 0.0\n",
    "image_syn_vis[image_syn_vis>1] = 1.0\n",
    "save_image(image_syn_vis, \"/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/111.png\", nrow=50) # Trying normalize = True/False may get better visual effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_real = []\n",
    "label_real = []\n",
    "for c in range(num_classes):\n",
    "    idx_shuffle = np.random.permutation(indices_class[c])[:50]\n",
    "    img_real.append(images_all[idx_shuffle].to(\"cpu\") )\n",
    "    label_real.append(labels_all[idx_shuffle].to(\"cpu\"))\n",
    "img_real = torch.from_numpy(np.concatenate(img_real, axis=0))\n",
    "label_real = torch.from_numpy(np.concatenate(label_real, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "# [2023-10-06 20:45:24] Evaluate_00: epoch = 1000 train time = 61 s train loss = 0.000965 train acc = 1.0000, test acc = 0.5001\n",
    "# Evaluate 1 random ConvNet, mean = 0.5001 std = 0.0000\n",
    "net_eval = get_network(model_eval, channel, num_classes, im_size).to(args.device) # get a random model\n",
    "image_syn_eval, label_syn_eval = copy.deepcopy(img_real), copy.deepcopy(label_real) # avoid any unaware modification\n",
    "_, acc_train, acc_test = evaluate_synset(it_eval, net_eval, image_syn_eval, label_syn_eval, testloader, args)\n",
    "accs.append(acc_test)\n",
    "print('Evaluate %d random %s, mean = %.4f std = %.4f\\n-------------------------'%(len(accs), model_eval, np.mean(accs), np.std(accs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ztl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
