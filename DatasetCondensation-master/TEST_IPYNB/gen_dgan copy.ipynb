{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "# import inception_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(img_syn, 'img_syn.pt')\n",
    "# torch.save(label_syn, 'label_syn.pt')\n",
    "# # 读取tensor\n",
    "img_syn = torch.load('img_syn.pt')\n",
    "label_syn = torch.load('label_syn.pt')\n",
    "\n",
    "device  = \"cuda:0\"\n",
    "img_syn =img_syn.to(device)\n",
    "label_syn =label_syn.to(device)\n",
    "device = img_syn.device\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='Parameter Processing')\n",
    "parser.add_argument('--dataset', type=str, default='CIFAR10', help='dataset')\n",
    "parser.add_argument('--model', type=str, default='ConvNet', help='model')\n",
    "parser.add_argument('--ipc', type=int, default=50, help='image(s) per class')\n",
    "parser.add_argument('--eval_mode', type=str, default='SS', help='eval_mode') # S: the same to training model, M: multi architectures,  W: net width, D: net depth, A: activation function, P: pooling layer, N: normalization layer,\n",
    "parser.add_argument('--num_exp', type=int, default=1, help='the number of experiments')\n",
    "parser.add_argument('--num_eval', type=int, default=1, help='the number of evaluating randomly initialized models')\n",
    "parser.add_argument('--epoch_eval_train', type=int, default=1000, help='epochs to train a model with synthetic data') # it can be small for speeding up with little performance drop\n",
    "parser.add_argument('--Iteration', type=int, default=2000, help='training iterations')\n",
    "parser.add_argument('--lr_img', type=float, default=1.0, help='learning rate for updating synthetic images')\n",
    "parser.add_argument('--lr_net', type=float, default=0.01, help='learning rate for updating network parameters')\n",
    "parser.add_argument('--batch_real', type=int, default=256, help='batch size for real data')\n",
    "parser.add_argument('--batch_train', type=int, default=256, help='batch size for training networks')\n",
    "parser.add_argument('--init', type=str, default='real', help='noise/real: initialize synthetic images from random noise or randomly sampled real images.')\n",
    "parser.add_argument('--dsa_strategy', type=str, default='color_crop_cutout_flip_scale_rotate', help='differentiable Siamese augmentation strategy')\n",
    "parser.add_argument('--data_path', type=str, default='/home/ssd7T/ZTL_gcond/data_cv', help='dataset path')\n",
    "parser.add_argument('--save_path', type=str, default='result/gen', help='path to save results')\n",
    "parser.add_argument('--dis_metric', type=str, default='ours', help='distance metric')\n",
    "from utils import get_loops, get_dataset, get_network, get_eval_pool, evaluate_synset, get_daparam, match_loss, get_time, TensorDataset, epoch, DiffAugment, ParamDiffAug\n",
    "import warnings\n",
    "args = parser.parse_args([])\n",
    "channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = get_dataset(args.dataset, args.data_path)\n",
    "\n",
    "images_all = [torch.unsqueeze(dst_train[i][0], dim=0) for i in range(len(dst_train))]\n",
    "labels_all = [dst_train[i][1] for i in range(len(dst_train))]\n",
    "indices_class = [[] for c in range(num_classes)]\n",
    "for i, lab in enumerate(labels_all):\n",
    "    indices_class[lab].append(i)\n",
    "images_all = torch.cat(images_all, dim=0).to(device)\n",
    "labels_all = torch.tensor(labels_all, dtype=torch.long, device=device)\n",
    "\n",
    "accs = []\n",
    "model_eval_pool = get_eval_pool(args.eval_mode, args.model, args.model)\n",
    "args.device = \"cuda:0\"\n",
    "import copy\n",
    "accs_all_exps = dict() # record performances of all experiments\n",
    "for key in model_eval_pool:\n",
    "    accs_all_exps[key] = []\n",
    "args.dsa_param = ParamDiffAug()\n",
    "args.dsa = False if args.dsa_strategy in ['none', 'None'] else True\n",
    "model_eval= model_eval_pool[0]\n",
    "data_save = []\n",
    "\n",
    "# img_real = []\n",
    "# label_real = []\n",
    "# for c in range(num_classes):\n",
    "#     idx_shuffle = np.random.permutation(indices_class[c])\n",
    "#     img_real.append(images_all[idx_shuffle].to(\"cpu\") )\n",
    "#     label_real.append(labels_all[idx_shuffle].to(\"cpu\"))\n",
    "# img_real = torch.from_numpy(np.concatenate(img_real, axis=0))\n",
    "# label_real = torch.from_numpy(np.concatenate(label_real, axis=0))\n",
    "\n",
    "\n",
    "SEED = 114514\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "img_real_train = []\n",
    "label_real_train = []\n",
    "for c in range(num_classes):\n",
    "    idx_shuffle = np.random.permutation(indices_class[c])[:50]\n",
    "    img_real_train.append(images_all[idx_shuffle].to(\"cpu\") )\n",
    "    label_real_train.append(labels_all[idx_shuffle].to(\"cpu\"))\n",
    "img_real_train = torch.from_numpy(np.concatenate(img_real_train, axis=0))\n",
    "label_real_train = torch.from_numpy(np.concatenate(label_real_train, axis=0))\n",
    "\n",
    "SEED = 87\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "img_real_test = []\n",
    "label_real_test = []\n",
    "for c in range(num_classes):\n",
    "    idx_shuffle = np.random.permutation(indices_class[c])[:50]\n",
    "    img_real_test.append(images_all[idx_shuffle].to(\"cpu\") )\n",
    "    label_real_test.append(labels_all[idx_shuffle].to(\"cpu\"))\n",
    "img_real_test = torch.from_numpy(np.concatenate(img_real_test, axis=0))\n",
    "label_real_test = torch.from_numpy(np.concatenate(label_real_test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils, torch, time, os, pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "# from dataloader import dataloader\n",
    "\n",
    "class generator(nn.Module):\n",
    "    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
    "    # Architecture : FC1024_BR-FC7x7x128_BR-(64)4dc2s_BR-(1)4dc2s_S\n",
    "    def __init__(self, input_dim=100, output_dim=1, input_size=32):\n",
    "        super(generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 128 * (self.input_size // 4) * (self.input_size // 4)),\n",
    "            nn.BatchNorm1d(128 * (self.input_size // 4) * (self.input_size // 4)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, self.output_dim, 4, 2, 1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        utils.initialize_weights(self)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.fc(input)\n",
    "        x = x.view(-1, 128, (self.input_size // 4), (self.input_size // 4))\n",
    "        x = self.deconv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class discriminator(nn.Module):\n",
    "    # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
    "    # Architecture : (64)4c2s-(128)4c2s_BL-FC1024_BL-FC1_S\n",
    "    def __init__(self, input_dim=1, output_dim=1, input_size=32):\n",
    "        super(discriminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.input_dim, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * (self.input_size // 4) * (self.input_size // 4), 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, self.output_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        utils.initialize_weights(self)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv(input)\n",
    "        x = x.view(-1, 128 * (self.input_size // 4) * (self.input_size // 4))\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DRAGAN(object):\n",
    "    def __init__(self, args):\n",
    "        # parameters\n",
    "        self.epoch = args.epoch\n",
    "        self.sample_num = 100\n",
    "        self.batch_size = args.batch_size\n",
    "        self.save_dir = args.save_dir\n",
    "        self.result_dir = args.result_dir\n",
    "        self.dataset = args.dataset\n",
    "        self.log_dir = args.log_dir\n",
    "        self.gpu_mode = args.gpu_mode\n",
    "        self.model_name = args.gan_type\n",
    "        self.input_size = args.input_size\n",
    "        self.z_dim = 62\n",
    "        self.lambda_ = 0.25\n",
    "\n",
    "        # load dataset\n",
    "        self.data_loader = dataloader(self.dataset, self.input_size, self.batch_size)\n",
    "        data = self.data_loader.__iter__().__next__()[0]\n",
    "\n",
    "        # networks init\n",
    "        self.G = generator(input_dim=self.z_dim, output_dim=data.shape[1], input_size=self.input_size)\n",
    "        self.D = discriminator(input_dim=data.shape[1], output_dim=1, input_size=self.input_size)\n",
    "        self.G_optimizer = optim.Adam(self.G.parameters(), lr=args.lrG, betas=(args.beta1, args.beta2))\n",
    "        self.D_optimizer = optim.Adam(self.D.parameters(), lr=args.lrD, betas=(args.beta1, args.beta2))\n",
    "\n",
    "        if self.gpu_mode:\n",
    "            self.G.cuda()\n",
    "            self.D.cuda()\n",
    "            self.BCE_loss = nn.BCELoss().cuda()\n",
    "        else:\n",
    "            self.BCE_loss = nn.BCELoss()\n",
    "\n",
    "        print('---------- Networks architecture -------------')\n",
    "        utils.print_network(self.G)\n",
    "        utils.print_network(self.D)\n",
    "        print('-----------------------------------------------')\n",
    "\n",
    "        # fixed noise\n",
    "        self.sample_z_ = torch.rand((self.batch_size, self.z_dim))\n",
    "        if self.gpu_mode:\n",
    "            self.sample_z_ = self.sample_z_.cuda()\n",
    "\n",
    "    def train(self):\n",
    "        self.train_hist = {}\n",
    "        self.train_hist['D_loss'] = []\n",
    "        self.train_hist['G_loss'] = []\n",
    "        self.train_hist['per_epoch_time'] = []\n",
    "        self.train_hist['total_time'] = []\n",
    "\n",
    "        self.y_real_, self.y_fake_ = torch.ones(self.batch_size, 1), torch.zeros(self.batch_size, 1)\n",
    "        if self.gpu_mode:\n",
    "            self.y_real_, self.y_fake_ = self.y_real_.cuda(), self.y_fake_.cuda()\n",
    "\n",
    "        self.D.train()\n",
    "        print('training start!!')\n",
    "        start_time = time.time()\n",
    "        for epoch in range(self.epoch):\n",
    "            epoch_start_time = time.time()\n",
    "            self.G.train()\n",
    "            for iter, (x_, _) in enumerate(self.data_loader):\n",
    "                if iter == self.data_loader.dataset.__len__() // self.batch_size:\n",
    "                    break\n",
    "\n",
    "                z_ = torch.rand((self.batch_size, self.z_dim))\n",
    "                if self.gpu_mode:\n",
    "                    x_, z_ = x_.cuda(), z_.cuda()\n",
    "\n",
    "                # update D network\n",
    "                self.D_optimizer.zero_grad()\n",
    "\n",
    "                D_real = self.D(x_)\n",
    "                D_real_loss = self.BCE_loss(D_real, self.y_real_)\n",
    "\n",
    "                G_ = self.G(z_)\n",
    "                D_fake = self.D(G_)\n",
    "                D_fake_loss = self.BCE_loss(D_fake, self.y_fake_)\n",
    "\n",
    "                \"\"\" DRAGAN Loss (Gradient penalty) \"\"\"\n",
    "                # This is borrowed from https://github.com/kodalinaveen3/DRAGAN/blob/master/DRAGAN.ipynb\n",
    "                alpha = torch.rand(self.batch_size, 1, 1, 1).cuda()\n",
    "                if self.gpu_mode:\n",
    "                    alpha = alpha.cuda()\n",
    "                    x_p = x_ + 0.5 * x_.std() * torch.rand(x_.size()).cuda()\n",
    "                else:\n",
    "                    x_p = x_ + 0.5 * x_.std() * torch.rand(x_.size())\n",
    "                differences = x_p - x_\n",
    "                interpolates = x_ + (alpha * differences)\n",
    "                interpolates.requires_grad = True\n",
    "                pred_hat = self.D(interpolates)\n",
    "                if self.gpu_mode:\n",
    "                    gradients = grad(outputs=pred_hat, inputs=interpolates, grad_outputs=torch.ones(pred_hat.size()).cuda(),\n",
    "                                 create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "                else:\n",
    "                    gradients = grad(outputs=pred_hat, inputs=interpolates, grad_outputs=torch.ones(pred_hat.size()),\n",
    "                         create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "                gradient_penalty = self.lambda_ * ((gradients.view(gradients.size()[0], -1).norm(2, 1) - 1) ** 2).mean()\n",
    "\n",
    "                D_loss = D_real_loss + D_fake_loss + gradient_penalty\n",
    "                self.train_hist['D_loss'].append(D_loss.item())\n",
    "                D_loss.backward()\n",
    "                self.D_optimizer.step()\n",
    "\n",
    "                # update G network\n",
    "                self.G_optimizer.zero_grad()\n",
    "\n",
    "                G_ = self.G(z_)\n",
    "                D_fake = self.D(G_)\n",
    "\n",
    "                G_loss = self.BCE_loss(D_fake, self.y_real_)\n",
    "                self.train_hist['G_loss'].append(G_loss.item())\n",
    "\n",
    "                G_loss.backward()\n",
    "                self.G_optimizer.step()\n",
    "\n",
    "                if ((iter + 1) % 100) == 0:\n",
    "                    print(\"Epoch: [%2d] [%4d/%4d] D_loss: %.8f, G_loss: %.8f\" %\n",
    "                          ((epoch + 1), (iter + 1), self.data_loader.dataset.__len__() // self.batch_size, D_loss.item(), G_loss.item()))\n",
    "\n",
    "            self.train_hist['per_epoch_time'].append(time.time() - epoch_start_time)\n",
    "            with torch.no_grad():\n",
    "                self.visualize_results((epoch+1))\n",
    "\n",
    "        self.train_hist['total_time'].append(time.time() - start_time)\n",
    "        print(\"Avg one epoch time: %.2f, total %d epochs time: %.2f\" % (np.mean(self.train_hist['per_epoch_time']),\n",
    "              self.epoch, self.train_hist['total_time'][0]))\n",
    "        print(\"Training finish!... save training results\")\n",
    "\n",
    "        self.save()\n",
    "        utils.generate_animation(self.result_dir + '/' + self.dataset + '/' + self.model_name + '/' + self.model_name, self.epoch)\n",
    "        utils.loss_plot(self.train_hist, os.path.join(self.save_dir, self.dataset, self.model_name), self.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=desc)\n",
    "\n",
    "\n",
    "parser.add_argument('--dataset', type=str, default='mnist', choices=['mnist', 'fashion-mnist', 'cifar10', 'cifar100', 'svhn', 'stl10', 'lsun-bed'],\n",
    "                    help='The name of dataset')\n",
    "parser.add_argument('--split', type=str, default='', help='The split flag for svhn and stl10')\n",
    "parser.add_argument('--epoch', type=int, default=50, help='The number of epochs to run')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='The size of batch')\n",
    "parser.add_argument('--input_size', type=int, default=28, help='The size of input image')\n",
    "parser.add_argument('--save_dir', type=str, default='models',\n",
    "                    help='Directory name to save the model')\n",
    "parser.add_argument('--result_dir', type=str, default='results', help='Directory name to save the generated images')\n",
    "parser.add_argument('--log_dir', type=str, default='logs', help='Directory name to save training logs')\n",
    "parser.add_argument('--lrG', type=float, default=0.0002)\n",
    "parser.add_argument('--lrD', type=float, default=0.0002)\n",
    "parser.add_argument('--beta1', type=float, default=0.5)\n",
    "parser.add_argument('--beta2', type=float, default=0.999)\n",
    "parser.add_argument('--gpu_mode', type=bool, default=True)\n",
    "parser.add_argument('--benchmark_mode', type=bool, default=True)\n",
    "\n",
    " args = parser.parse_args[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = DRAGAN(args)\n",
    "\n",
    "gan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 8, 8])\n",
      "torch.Size([1, 128, 16, 16])\n",
      "torch.Size([1, 64, 32, 32])\n",
      "torch.Size([1, 3, 64, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (64) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_dgan copy.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_dgan%20copy.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(fake\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_dgan%20copy.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Gradient penalty\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_dgan%20copy.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m penalty \u001b[39m=\u001b[39m gradient_penalty(batch_img_syn, fake)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_dgan%20copy.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Adversarial loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_dgan%20copy.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m loss_D \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mmean(D(batch_img)) \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mmean(D(fake)) \u001b[39m+\u001b[39m lambd\u001b[39m*\u001b[39mpenalty\n",
      "\u001b[1;32m/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_dgan copy.ipynb Cell 7\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_dgan%20copy.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgradient_penalty\u001b[39m(real_samples, fake_samples):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_dgan%20copy.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m     alpha \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(batch_size, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_dgan%20copy.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m     interpolates \u001b[39m=\u001b[39m (alpha \u001b[39m*\u001b[39;49m real_samples \u001b[39m+\u001b[39;49m ((\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m alpha) \u001b[39m*\u001b[39;49m fake_samples))\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_dgan%20copy.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m     d_interpolates \u001b[39m=\u001b[39m D(interpolates)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_dgan%20copy.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m     fake \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull((batch_size, \u001b[39m1\u001b[39m), \u001b[39m1\u001b[39m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat, device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (64) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs): \n",
    " \n",
    "    for c in range(num_classes):\n",
    "    \n",
    "        batch_img = img_real_train[c*batch_size:(c+1)*batch_size].reshape((batch_size, 3, 32, 32)).to(device).to(device)\n",
    "        batch_img_label = label_real_train[c*batch_size:(c+1)*batch_size].to(device)\n",
    "        batch_img_syn = img_syn[c*batch_size:(c+1)*batch_size].reshape((batch_size, 3, 32, 32)).to(device).to(device)\n",
    "        batch_img_syn_label = label_syn[c*batch_size:(c+1)*batch_size].to(device)\n",
    "\n",
    "        ## Train Discriminator \n",
    "        # noise = torch.randn(batch_size, z_dim, 1, 1).to(device)\n",
    "        fake = G(batch_img)\n",
    "        # print(batch_img_syn.shape)\n",
    "        print(fake.shape)\n",
    "\n",
    "        # Gradient penalty\n",
    "        penalty = gradient_penalty(batch_img_syn, fake)\n",
    "\n",
    "        # Adversarial loss\n",
    "        loss_D = -torch.mean(D(batch_img)) + torch.mean(D(fake)) + lambd*penalty\n",
    "        \n",
    "        opt_D.zero_grad()\n",
    "        loss_D.backward(retain_graph=True) \n",
    "        opt_D.step()\n",
    "        \n",
    "        ## Train Generator\n",
    "        loss_G = -torch.mean(D(fake))\n",
    "        \n",
    "        opt_G.zero_grad()\n",
    "        loss_G.backward()\n",
    "        opt_G.step()\n",
    "\n",
    "        # Print losses  \n",
    "        print(\n",
    "            \"[Epoch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "            % (epoch, n_epochs, loss_D.item(), loss_G.item())\n",
    "        )\n",
    "\n",
    "        # if batch_idx % 100 == 0:\n",
    "        #     step += 1\n",
    "        #     writer.add_scalar(\"Discriminator loss\", loss_D.item(), global_step=step)\n",
    "        #     writer.add_scalar(\"Generator loss\", loss_G.item(), global_step=step)\n",
    "        #     with torch.no_grad():\n",
    "        #         fake = G(noise)\n",
    "        #         img_grid_fake = make_grid(fake[:32], normalize=True)\n",
    "            \n",
    "        #         writer.add_image(\"Fake images\", img_grid_fake, global_step=step)\n",
    "            \n",
    "print(\"Training finished!\")\n",
    "\n",
    "#  (64, 3, 4, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ztl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
