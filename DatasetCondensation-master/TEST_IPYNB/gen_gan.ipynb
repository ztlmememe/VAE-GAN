{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from IPython import embed\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# OS\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "model_urls = {\n",
    "    'cifar10': 'http://ml.cs.tsinghua.edu.cn/~chenxi/pytorch-models/cifar10-d875770b.pth',\n",
    "    'cifar100': 'http://ml.cs.tsinghua.edu.cn/~chenxi/pytorch-models/cifar100-3a55a987.pth',\n",
    "}\n",
    "\n",
    "class CIFAR(nn.Module):\n",
    "    def __init__(self, features, n_channel, num_classes):\n",
    "        super(CIFAR, self).__init__()\n",
    "        assert isinstance(features, nn.Sequential), type(features)\n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(n_channel, num_classes)\n",
    "        )\n",
    "        # print(self.features)\n",
    "        # print(self.classifier)\n",
    "    def embedding(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for i, v in enumerate(cfg):\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            padding = v[1] if isinstance(v, tuple) else 1\n",
    "            out_channels = v[0] if isinstance(v, tuple) else v\n",
    "            conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=padding)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(out_channels, affine=False), nn.ReLU()]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU()]\n",
    "            in_channels = out_channels\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def cifar10(n_channel, pretrained=None):\n",
    "    cfg = [n_channel, n_channel, 'M', 2*n_channel, 2*n_channel, 'M', 4*n_channel, 4*n_channel, 'M', (8*n_channel, 0), 'M']\n",
    "    layers = make_layers(cfg, batch_norm=True)\n",
    "    model = CIFAR(layers, n_channel=8*n_channel, num_classes=10)\n",
    "    if pretrained is not None:\n",
    "        m = model_zoo.load_url(model_urls['cifar10'])\n",
    "        state_dict = m.state_dict() if isinstance(m, nn.Module) else m\n",
    "        assert isinstance(state_dict, (dict, OrderedDict)), type(state_dict)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "def cifar100(n_channel, pretrained=None):\n",
    "    cfg = [n_channel, n_channel, 'M', 2*n_channel, 2*n_channel, 'M', 4*n_channel, 4*n_channel, 'M', (8*n_channel, 0), 'M']\n",
    "    layers = make_layers(cfg, batch_norm=True)\n",
    "    model = CIFAR(layers, n_channel=8*n_channel, num_classes=100)\n",
    "    if pretrained is not None:\n",
    "        m = model_zoo.load_url(model_urls['cifar100'])\n",
    "        state_dict = m.state_dict() if isinstance(m, nn.Module) else m\n",
    "        assert isinstance(state_dict, (dict, OrderedDict)), type(state_dict)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gan_model = cifar10(128, pretrained='log/cifar10/best-135.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(img_syn, 'img_syn.pt')\n",
    "# torch.save(label_syn, 'label_syn.pt')\n",
    "# # 读取tensor\n",
    "img_syn = torch.load('img_syn.pt')\n",
    "label_syn = torch.load('label_syn.pt')\n",
    "\n",
    "device  = \"cuda:0\"\n",
    "img_syn =img_syn.to(device)\n",
    "label_syn =label_syn.to(device)\n",
    "device = img_syn.device\n",
    "\n",
    "# Create model\n",
    "gan_model = cifar10(128).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='Parameter Processing')\n",
    "parser.add_argument('--dataset', type=str, default='CIFAR10', help='dataset')\n",
    "parser.add_argument('--model', type=str, default='ConvNet', help='model')\n",
    "parser.add_argument('--ipc', type=int, default=50, help='image(s) per class')\n",
    "parser.add_argument('--eval_mode', type=str, default='SS', help='eval_mode') # S: the same to training model, M: multi architectures,  W: net width, D: net depth, A: activation function, P: pooling layer, N: normalization layer,\n",
    "parser.add_argument('--num_exp', type=int, default=1, help='the number of experiments')\n",
    "parser.add_argument('--num_eval', type=int, default=1, help='the number of evaluating randomly initialized models')\n",
    "parser.add_argument('--epoch_eval_train', type=int, default=1000, help='epochs to train a model with synthetic data') # it can be small for speeding up with little performance drop\n",
    "parser.add_argument('--Iteration', type=int, default=2000, help='training iterations')\n",
    "parser.add_argument('--lr_img', type=float, default=1.0, help='learning rate for updating synthetic images')\n",
    "parser.add_argument('--lr_net', type=float, default=0.01, help='learning rate for updating network parameters')\n",
    "parser.add_argument('--batch_real', type=int, default=256, help='batch size for real data')\n",
    "parser.add_argument('--batch_train', type=int, default=256, help='batch size for training networks')\n",
    "parser.add_argument('--init', type=str, default='real', help='noise/real: initialize synthetic images from random noise or randomly sampled real images.')\n",
    "parser.add_argument('--dsa_strategy', type=str, default='color_crop_cutout_flip_scale_rotate', help='differentiable Siamese augmentation strategy')\n",
    "parser.add_argument('--data_path', type=str, default='/home/ssd7T/ZTL_gcond/data_cv', help='dataset path')\n",
    "parser.add_argument('--save_path', type=str, default='result/gen', help='path to save results')\n",
    "parser.add_argument('--dis_metric', type=str, default='ours', help='distance metric')\n",
    "from utils import get_loops, get_dataset, get_network, get_eval_pool, evaluate_synset, get_daparam, match_loss, get_time, TensorDataset, epoch, DiffAugment, ParamDiffAug\n",
    "import warnings\n",
    "args = parser.parse_args([])\n",
    "channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = get_dataset(args.dataset, args.data_path)\n",
    "\n",
    "images_all = [torch.unsqueeze(dst_train[i][0], dim=0) for i in range(len(dst_train))]\n",
    "labels_all = [dst_train[i][1] for i in range(len(dst_train))]\n",
    "indices_class = [[] for c in range(num_classes)]\n",
    "for i, lab in enumerate(labels_all):\n",
    "    indices_class[lab].append(i)\n",
    "images_all = torch.cat(images_all, dim=0).to(device)\n",
    "labels_all = torch.tensor(labels_all, dtype=torch.long, device=device)\n",
    "\n",
    "accs = []\n",
    "model_eval_pool = get_eval_pool(args.eval_mode, args.model, args.model)\n",
    "args.device = \"cuda:0\"\n",
    "import copy\n",
    "accs_all_exps = dict() # record performances of all experiments\n",
    "for key in model_eval_pool:\n",
    "    accs_all_exps[key] = []\n",
    "args.dsa_param = ParamDiffAug()\n",
    "args.dsa = False if args.dsa_strategy in ['none', 'None'] else True\n",
    "model_eval= model_eval_pool[0]\n",
    "data_save = []\n",
    "\n",
    "# img_real = []\n",
    "# label_real = []\n",
    "# for c in range(num_classes):\n",
    "#     idx_shuffle = np.random.permutation(indices_class[c])\n",
    "#     img_real.append(images_all[idx_shuffle].to(\"cpu\") )\n",
    "#     label_real.append(labels_all[idx_shuffle].to(\"cpu\"))\n",
    "# img_real = torch.from_numpy(np.concatenate(img_real, axis=0))\n",
    "# label_real = torch.from_numpy(np.concatenate(label_real, axis=0))\n",
    "\n",
    "\n",
    "SEED = 114514\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "img_real_train = []\n",
    "label_real_train = []\n",
    "for c in range(num_classes):\n",
    "    idx_shuffle = np.random.permutation(indices_class[c])\n",
    "    img_real_train.append(images_all[idx_shuffle].to(\"cpu\") )\n",
    "    label_real_train.append(labels_all[idx_shuffle].to(\"cpu\"))\n",
    "img_real_train = torch.from_numpy(np.concatenate(img_real_train, axis=0))\n",
    "label_real_train = torch.from_numpy(np.concatenate(label_real_train, axis=0))\n",
    "\n",
    "SEED = 87\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "img_real_test = []\n",
    "label_real_test = []\n",
    "for c in range(num_classes):\n",
    "    idx_shuffle = np.random.permutation(indices_class[c])[:50]\n",
    "    img_real_test.append(images_all[idx_shuffle].to(\"cpu\") )\n",
    "    label_real_test.append(labels_all[idx_shuffle].to(\"cpu\"))\n",
    "img_real_test = torch.from_numpy(np.concatenate(img_real_test, axis=0))\n",
    "label_real_test = torch.from_numpy(np.concatenate(label_real_test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "# from utee import misc\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from IPython import embed\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR-X Example')\n",
    "parser.add_argument('--type', default='cifar10', help='cifar10|cifar100')\n",
    "parser.add_argument('--channel', type=int, default=128, help='first conv channel (default: 32)')\n",
    "parser.add_argument('--wd', type=float, default=0.00, help='weight decay')\n",
    "parser.add_argument('--batch_size', type=int, default=50, help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--epochs', type=int, default=150, help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='learning rate (default: 1e-3)')\n",
    "\n",
    "parser.add_argument('--log_interval', type=int, default=100,  help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--test_interval', type=int, default=5,  help='how many epochs to wait before another test')\n",
    "\n",
    "parser.add_argument('--decreasing_lr', default='80,120', help='decreasing strategy')\n",
    "args = parser.parse_args([])\n",
    "SEED = 87\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decreasing_lr: [80, 120]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "num_classes = 10\n",
    "batch = 50\n",
    "num_feat = 3072\n",
    "criterion = nn.BCELoss()\n",
    "# model = Autoencoder(num_feat).to(device)  \n",
    "# 训练\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(gan_model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
    "decreasing_lr = list(map(int, args.decreasing_lr.split(',')))\n",
    "print('decreasing_lr: ' + str(decreasing_lr))\n",
    "\n",
    "\n",
    "best_acc, old_file = 0, None\n",
    "t_begin = time.time()\n",
    "\n",
    "# ready to go\n",
    "for epoch in range(2000):\n",
    "  \n",
    "    gan_model.train()\n",
    "    if epoch in decreasing_lr:\n",
    "        optimizer.param_groups[0]['lr'] *= 0.1\n",
    "    for c in range(num_classes):\n",
    "        batch_img = img_real_train[c*batch:(c+1)*batch].reshape((batch, 3, 32, 32)).to(device) \n",
    "        # label_real_train = label_real_train.to(device) \n",
    "        batch_label = label_syn[c*batch:(c+1)*batch].to(device) \n",
    "        # batch_img = batch_img.to(device) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = gan_model(batch_img)\n",
    "        loss = F.cross_entropy(output, batch_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#   output = autoencoder(img_real_test.to(device))\n",
    "  \n",
    "  \n",
    "  \n",
    "gan_model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "output = gan_model.embedding(img_real_test.to(device))\n",
    "    # test_loss += F.cross_entropy(output, target).data[0]\n",
    "    # pred = output.data.max(1)[1]  # get the index of the max log-probability\n",
    "    # correct += pred.cpu().eq(indx_target).sum()\n",
    "\n",
    "    #         test_loss = test_loss / len(test_loader) # average over number of mini-batch\n",
    "    #         acc = 100. * correct / len(test_loader.dataset)\n",
    "    #         print('\\tTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "    #             test_loss, correct, len(test_loader.dataset), acc))\n",
    "    #         if acc > best_acc:\n",
    "    #             new_file = os.path.join(args.logdir, 'best-{}.pth'.format(epoch))\n",
    "    #             misc.model_snapshot(model, new_file, old_file=old_file, verbose=True)\n",
    "    #             best_acc = acc\n",
    "    #             old_file = new_file\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = gan_model(img_real_test.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = output.data.max(1)[1]  # get the index of the max log-probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct += pred.cpu().eq(label_real_test).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Accuracy: 9%\n"
     ]
    }
   ],
   "source": [
    "acc = 100. * correct / len(output)\n",
    "print('Test set: Accuracy: {:.0f}%'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accs = []\n",
    "model_eval_pool = get_eval_pool(args.eval_mode, args.model, args.model)\n",
    "args.device = \"cuda:0\"\n",
    "import copy\n",
    "accs_all_exps = dict() # record performances of all experiments\n",
    "for key in model_eval_pool:\n",
    "    accs_all_exps[key] = []\n",
    "args.dsa_param = ParamDiffAug()\n",
    "args.dsa = False if args.dsa_strategy in ['none', 'None'] else True\n",
    "model_eval= model_eval_pool[0]\n",
    "\n",
    "# [2023-10-06 20:45:24] Evaluate_00: epoch = 1000 train time = 61 s train loss = 0.000965 train acc = 1.0000, test acc = 0.5001\n",
    "# Evaluate 1 random ConvNet, mean = 0.5001 std = 0.0000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_gan.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_gan.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m net_eval \u001b[39m=\u001b[39m get_network(model_eval, channel, num_classes, im_size)\u001b[39m.\u001b[39mto(device) \u001b[39m# get a random model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_gan.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m image_syn_eval, label_syn_eval \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(output\u001b[39m.\u001b[39mdetach()), copy\u001b[39m.\u001b[39mdeepcopy(label_real_test) \u001b[39m# avoid any unaware modification\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_gan.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m _, acc_train, acc_test \u001b[39m=\u001b[39m evaluate_synset(\u001b[39m1\u001b[39;49m, net_eval, image_syn_eval, label_syn_eval, testloader, args)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_gan.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m accs\u001b[39m.\u001b[39mappend(acc_test)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e55532d56313030227d/home/wangkai/ztl_project/difussion-dd/DatasetCondensation-master/gen_gan.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m()\n",
      "File \u001b[0;32m~/ztl_project/difussion-dd/DatasetCondensation-master/utils.py:352\u001b[0m, in \u001b[0;36mevaluate_synset\u001b[0;34m(it_eval, net, images_train, labels_train, testloader, args)\u001b[0m\n\u001b[1;32m    350\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    351\u001b[0m \u001b[39mfor\u001b[39;00m ep \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(Epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m--> 352\u001b[0m     loss_train, acc_train \u001b[39m=\u001b[39m epoch(\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, trainloader, net, optimizer, criterion, args, aug \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    353\u001b[0m     \u001b[39mif\u001b[39;00m ep \u001b[39min\u001b[39;00m lr_schedule:\n\u001b[1;32m    354\u001b[0m         lr \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m\n",
      "File \u001b[0;32m~/ztl_project/difussion-dd/DatasetCondensation-master/utils.py:311\u001b[0m, in \u001b[0;36mepoch\u001b[0;34m(mode, dataloader, net, optimizer, criterion, args, aug)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39mif\u001b[39;00m aug:\n\u001b[1;32m    310\u001b[0m     \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mdsa:\n\u001b[0;32m--> 311\u001b[0m         img \u001b[39m=\u001b[39m DiffAugment(img, args\u001b[39m.\u001b[39;49mdsa_strategy, param\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mdsa_param)\n\u001b[1;32m    312\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m         img \u001b[39m=\u001b[39m augment(img, args\u001b[39m.\u001b[39mdc_aug_param, device\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/ztl_project/difussion-dd/DatasetCondensation-master/utils.py:516\u001b[0m, in \u001b[0;36mDiffAugment\u001b[0;34m(x, strategy, seed, param)\u001b[0m\n\u001b[1;32m    514\u001b[0m     p \u001b[39m=\u001b[39m pbties[torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(pbties), size\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,))\u001b[39m.\u001b[39mitem()]\n\u001b[1;32m    515\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m AUGMENT_FNS[p]:\n\u001b[0;32m--> 516\u001b[0m         x \u001b[39m=\u001b[39m f(x, param)\n\u001b[1;32m    517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     exit(\u001b[39m'\u001b[39m\u001b[39munknown augmentation mode: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m%\u001b[39mparam\u001b[39m.\u001b[39maug_mode)\n",
      "File \u001b[0;32m~/ztl_project/difussion-dd/DatasetCondensation-master/utils.py:600\u001b[0m, in \u001b[0;36mrand_crop\u001b[0;34m(x, param)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrand_crop\u001b[39m(x, param):\n\u001b[1;32m    598\u001b[0m     \u001b[39m# The image is padded on its surrounding and then cropped.\u001b[39;00m\n\u001b[1;32m    599\u001b[0m     ratio \u001b[39m=\u001b[39m param\u001b[39m.\u001b[39mratio_crop_pad\n\u001b[0;32m--> 600\u001b[0m     shift_x, shift_y \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(x\u001b[39m.\u001b[39;49msize(\u001b[39m2\u001b[39;49m) \u001b[39m*\u001b[39m ratio \u001b[39m+\u001b[39m \u001b[39m0.5\u001b[39m), \u001b[39mint\u001b[39m(x\u001b[39m.\u001b[39msize(\u001b[39m3\u001b[39m) \u001b[39m*\u001b[39m ratio \u001b[39m+\u001b[39m \u001b[39m0.5\u001b[39m)\n\u001b[1;32m    601\u001b[0m     set_seed_DiffAug(param)\n\u001b[1;32m    602\u001b[0m     translation_x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39m-\u001b[39mshift_x, shift_x \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, size\u001b[39m=\u001b[39m[x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m], device\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "net_eval = get_network(model_eval, channel, num_classes, im_size).to(device) # get a random model\n",
    "image_syn_eval, label_syn_eval = copy.deepcopy(output.detach()), copy.deepcopy(label_real_test) # avoid any unaware modification\n",
    "_, acc_train, acc_test = evaluate_synset(1, net_eval, image_syn_eval, label_syn_eval, testloader, args)\n",
    "accs.append(acc_test)\n",
    "print()\n",
    "print('Evaluate %d random %s, mean = %.4f std = %.4f\\n-------------------------'%(len(accs), model_eval, np.mean(accs), np.std(accs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ztl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
